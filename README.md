# LLM from Scratch — Personal Notes

This repo is my implementation of the book **"Build a Large Language Model from Scratch"**(Manning Publications). The goal is to understand how LLMs actually work by building each part step by step, not just using prebuilt libraries.

## Goal

Build an LLM from scratch by following the book, implementing:

- Tokenizer
- Data preparation
- Transformer model
- Training loop
- Inference

## Why

I want to:
- Deepen my understanding of transformers
- Practice building everything from low-level components
- Eventually train a small LLM and use it for fun experiments

## Structure (WIP)

```
llm-from-scratch/
├── data/           # Dataset and preprocessing
├── tokenizer/      # Custom tokenizer
├── model/          # Transformer
├── dataset/        # Dataset used for training
├── train.py        # Training script
├── inference.py    # Generation
└── README.md       # (this file)
```

## Notes

- Following the book closely, but I’ll try to experiment where it makes sense
